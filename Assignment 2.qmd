---
title: "A Descriptive Analysis of SONA speeches 1994-2023"
author: "Kenneth Ssekimpi & Levy Banda"
student_number: "SSKKEN001 & BNDLEV001"
assignment: "Assignment 2"
editor: visual
embed-resources: true
bibliography: Reference_list.bib
format: 
  html:
    toc: true
execute: 
  warning: false
  echo: false
  cache: true
metadata:
  link-citations: true
  date-format: long
  lang: en
---

```{r}
#| label: setup
#| include: false

knitr::opts_knit$set(root.dir = "C:/Users/User/OneDrive/Documents/School/2023/Masters/STA5073Z/Assignments/Assignment 2/ds4l-assignment-2")

knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
```

```{r}
#| label: packages
#| include: false

# Clear global environment
# rm(list=ls())

# Libraries we need
libs <- c('dplyr', 'ggplot2', 'kableExtra', 'lubridate', 'magick', 'purrr', 'reshape2', 'stringr', 'text2vec', 'tidyr', 'tidytext', 'topicdoc', 'topicmodels', 'tm', 'wordcloud')

# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
  install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}

# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
```

```{r}
#| label: unzip
#| inclde: false

unzip("sona-addresses-1994-2023.zip", exdir = "data")
```

# Abstract

------------------------------------------------------------------------

The State of the Nation Address (SONA) in South Africa is a pivotal annual event where the president outlines the government's achievements, plans, and priorities, serving as a vital platform for accountability and transparency. Researchers analyze SONAs using data mining techniques to gain insights into past speeches, assess government sentiment, identify key topics, and track the evolution of government priorities, contributing to a deeper understanding of this critical political discourse.

Sentiment analysis and Latent Dirichlet Allocation (LDA) are two data mining techniques that can be used to study political discourse. Sentiment analysis can be used to identify the emotional tone of text, while LDA can be used to identify thematic topics in large collections of text.

The research involved collecting and processing SONA speeches from 1994 to 2023, utilizing data preprocessing to make the text suitable for analysis. The analysis included sentiment analysis using various lexicons, LDA for topic modeling, and determining the optimal number of topics through coherence scores, ultimately selecting seven topics for more distinct and relevant results.

The analysis involves examining the frequency of positive and negative words in speeches by different South African presidents, highlighting how these trends change over time. It also evaluates the performance of an LDA model in distinguishing topics discussed in the speeches and how these topics are associated with different presidents, with specific attention to the LDA model's metrics, topic relevance over time, and their connection to presidential terms.

The lessons learned from this study can inform future research on natural language processing's use in political science.

------------------------------------------------------------------------

**Keywords**: sentiment analysis, latent dirichlet allocation, topic modeling, sona

# Introduction

The State of the Nation Address (SONA) in South Africa serves as an opportunity for the president to address the nation and provide an overview of the government's accomplishments, plans, and priorities. This speech marks the opening of Parliament. The SONA is an important political event in the South African political calendar, and plays an important role in accountability and transparency. It is a key moment for the president to communicate the government's agenda to the South African Parliament (National Assembly and National Council of Provinces) [@sagov].

The SONA was first held in 1990, a once-off event then, but has since the inauguration of President Nelson Mandela in 1994 been an annual event. Then outgoing President Frederik Willem de Klerk held a SONA before the election in 1994, setting a precedent for the event to be held twice in election years - once before and once after [@sagov].

At the SONA, the President of South Africa is able to not only inform the nation of the economic and political policies, social development, and security, but also encourage South Africans, bringing hope and inspiring confidence in government. For example, after the South African National Rugby team won the Rugby World Cup in 2019, President Cryril Ramaphosa congratulated the team on their achievement and highlighted the pride this brought to the country during his SONA at the beginning of 2020 [@sagov].

Democratic governance can be seen at SONA's, as the president exposes the problems the country is currently facing and possibly provides an account on what is being done to solve them [@eshbaugh2010politics]. The government's response to the most pressing issues varied over time, but the SONA remains an important mechanism for public engagement and debate.

The SONA - and other similar speeches made by presidents elsewhere - is analyzedd in a variety of ways by academics, journalists, and civil society organizations. An example being [@banguis2019role], who highlighted that speeches made by Asian presidents revealed a lot of information about the state of affairs in the periods studied. This analysis can be used to inform policy and decision-making, and to hold government accountable for its promises.

The implementation of data mining analysis techniques, such as topic modeling and sentiment analysis, allows for a more profound examination and comprehension of past SONAs. Data mining offers the opportunity to gain insights into the challenges faced by previous leaders and to evaluate the ongoing relevance of these issues in contemporary contexts [@miranda2021exploring].

In this paper, we use data mining techniques to analyze the content of past SONAs. We examine on the sentiment of the SONAs in order to assess the government's tone and outlook. We also focus on identifying the key topics that have been addressed in the past, and on tracking how the government's priorities have evolved over time.

Our research thus contributes to the existing literature on the SONA by providing a systematic analysis of the content of past speeches.

# Literature Review

Sentiment analysis and Latent Dirichlet Allocation (LDA) are two data mining techniques that can be used to study political discourse. Sentiment analysis can be used to identify the emotional tone of text, while LDA can be used to identify thematic topics in large collections of text.

## Sentiment Analysis

Sentiment analysis is a natural language processing (NLP) technique that aims to identify the sentiment of a piece of text [@chong2014]. Sentiment is defined as the emotional tone of a text, and it can be positive, negative, or neutral [@mohammad2016]. Sentiment analysis is used in a variety of applications, including social media monitoring, customer feedback analysis, and product development [@olivera2018].

Sentiment analysis can be performed using a variety of machine learning techniques [@chakraborty2018]. One common approach is to use a supervised learning technique, such as support vector machines (SVG) or logistic regression. Supervised learning techniques require a labeled dataset of text and sentiment labels. The model is trained on this dataset, and then used to predict the sentiment of new pieces of text.

Another common approach to sentiment analysis is to use an unsupervised learning technique. Unsupervised learning techniques do not require a labeled dataset. Instead, they learn the sentiment of text by identifying patterns in the data.

Sentiment analysis is a rapidly growing field of research, and new techniques are being developed all the time. [@taj2021] provided a comprehensive overview of the evolving methodologies of sentiment analysis, highlighting the key strengths and weaknesses of each approach. The approaches were defined as follows:

-   ***Lexicon-based sentiment analysis***: the oldest and most widely used approach to sentiment analysis. It relies on a dictionary of words and phrases, each of which is associated with a sentiment score. The sentiment score of a text is calculated by averaging the sentiment scores of the individual words and phrases it contains. It is simple to implement, efficient, and language independent, but it can be inaccurate, especially for complex or nuanced language [@taj2021].

-   ***Machine learning-based sentiment analysis***: a newer approach to sentiment analysis that uses machine learning algorithms to train a model to predict the sentiment score of a text. The model is trained on a dataset of labeled test examples, each of which has been assigned a sentiment score. Once the model is trained, it can be used to predict the sentiment of new texts. It can be more accurate than lexicon-based sentiment analysis, but it requires a large amount of labeled training data and is more computationally expensive. It may also be susceptible to overfitting, with the model performing well on training data, but not on new data [@taj2021].

-   ***Deep learning-based sentiment analysis***: is a type of machine learning-based sentiment analysis that uses deep neural networks to train a model to predict the sentiment of a text. Deep neural networks are able to learn complex patterns in data, which makes them well-suited for sentiment analysis tasks. This approach was seen by then researchers as the most accurate approach to sentiment analysis, but it requires a large amount of labeled training data and is computationally more expensive than the previous two approaches. They are also susceptible to overfitting [@taj2021].

Studies such as [@budiharto2018prediction] have suggested frameworks into using sentiment analysis on posts on relevant topics on some social media websites to predict election results. Sentiment analysis can also be used to determine the mood of the nation by monitoring announcements made by leaders of the country, including presidential candidates [@vo2017multi]. [@bringula2023youtube] analyzed comments on Youtube to analyze opinions from political vloggers about two presidential canditates. Sentiment analysis can be used to summarize political issues and current affairs [@bollen2011modeling]. [@miranda2021exploring] studied State of the Nation Addresses of 13 Philippine presidents and discovered that the speeches had huge focus towards economic development, enhancement of public services, and addressing issues faced by the nation.

## Latent Dirichlet Allocation

Latent Dirichlet Allocation (LDA) is a generative probabilistic model for collections of discrete data such as text corpora [@blei2003lda]. It is a topic modeling technique, which means that it can be used to identify thematic topics in large collections of text.

Topic modeling is thus an unsupervised learning technique that receives a collection of documents and produces topics as mathematical objects [@hamed2019]. Text topics describe the subject of the text. Topic modeling is trying to find similar topics across different documents, and trying to group different words together, such that each topic will consist of words with similar meanings [@blei2009]. LDA works by first assuming that each document is a mixture of topics, and then iteratively assigning each word in the document to a topic [@campbell2015]. The probability of a word being assigned to a topic is based on the distribution of words in that topic across all of the documents, and the distribution of topics in the document [@campbell2015].

```{r}
#| label: fig-lda-plate-diag
#| fig.cap: LDA Model Plate Diagram

img <- magick::image_read("C:/Users/User/OneDrive/Documents/School/2023/Masters/STA5073Z/Assignments/Assignment 2/ds4l-assignment-2/smoothed-lda.PNG")
plot(img)
```

[@wang2018] presented a plate diagram of an LDA model @fig-lda-plate-diag where:

-   $\alpha$: the per-document topic distributions (i.e. matrix of documents in rows and topics in columns),

-   $\beta$: the per-topic word distribution (i.e. matrix of topics in rows and words in columns),

-   $\theta$: the topic distribution for document *m*,

-   $\phi$: the word distribution for topic *k*,

-   *z:* the topic for the *n*-th word in document *m*, and

-   *w:* the specific word

The only observable variable in the LDA model is the word *w* in grey; each of the other variables is passive or *latent*. To tweak the model, we can adjust the $\alpha$ and $\beta$ parameters.\
$\alpha$ controls the distribution of topics across documents. A symmetric distribution would mean that all topics are evenly distributed in all documents, while an asymmetric distribution would favour certain topics over others [@wang2018].

$\beta$ controls the distribution of words within topics. Usually, each word is distributed evenly within a topic. However, we can bias certain topics to favour certain words. For example, in a topic about Apple products we can bias words such as *iphone* or *ipad* [@wang2018].

Some of strengths of LDA include:

-   Identifying topics distributed across multiple documents, and modeling relationships between topics.

-   Ease of implementation and efficient allocation of computing resources.

-   Language independence [@negara2019].

In contrast, some of its weaknesses include:

-   LDA's sensitivity to the number of specified topics. If the number of topics is too small, LDA may not be able to capture the full range of topics in the data. Conversely, if the number is too large, LDA may produce fragmented or incoherent topics.

-   LDA can be difficult to interpret.

-   LDA can be affected by data quality, especially for noisy or sparse data [@negara2019].

LDA has been used in a variety of applications, including: identifying key topics in a collection of news articles; understanding the sentiment of customer reviews; and grouping social media posts into different topics [@wahid2022].

While LDA is a relatively new tool in political science research, it has the potential to become an increasingly important tool for understanding political discourse and processes. It has been used in a variety of ways in political science research. For example, [@miranda20211exploring] used LDA to analyze the content of political speeches and texts to identify the key topics and themes. [@xu2020] used LDA to identify patterns in online discourse polarization and extremism, and the contributions and parameters thereof. [@brookes2019] used LDA to study the relationship between political discourse and political outcomes. [@mutanga2022] used LDA to identify the most topical issues relating to the COVID-19 pandemic that were being discussed by South Africans on Twitter, as well as the impacts of these issues on the people's reactions to and compliance with the South African government's efforts in managing the disease.

Sentiment analysis and LDA are both powerful tools for studying political discourse. However, it is important to be aware of the strengths and weaknesses of each method. Sentiment analysis can be susceptible to noise and bias, and LDA can be difficult to interpret. In this paper, we use sentiment analysis and LDA to to conduct a descriptive analysis of the content of the past SONA speeches. We focus on the sentiment of the discourse as well as on identifying key topics and themes.

# Data and Methodology

## Data Preprocessing

The full text of SONA speeches, from 1994 to 2023, was collected from the official South African Governement website. Data preprocessing is an essential phase in text analysis, aimed at transforming the raw text data into a structured format suitable for in-depth analysis of the speeches. For this we chose the 'tidytext' package found in R programming language which provided a versatile platform for text analysis

The initial step in data preprocessing was to identify the date of the speech and the president who addressed the nation. Following this was the removal of special characters, digits and punctuation marks. Elements, such as symbols, commas, periods, and question marks, can introduce noise into the text and hinder accurate analysis. Numerical values and digits hold limited relevance and may disrupt the analysis of textual content. Their exclusion simplifies the text, making it more text-centric.

To extract meaningful information from the text, tokenization is employed. It involves breaking down the continuous string of text into individual units, known as tokens, usually words or phrases. Tokenization provides the foundation for understanding the text's structure and content, making it more amenable to analysis. All the characters in the text are converted to lower case. This ensures that the analysis is not case sensitive. Stop words, commonly occurring words that hold not much meaning, are removed: shifting the focus to content-carrying words, enhancing the identification of significant themes and patterns in the SONA speeches.

```{r}
#| label: read_wrangle
#| warning: false

# Get a list of all text files in the directory
text_files <- list.files(path = "data", pattern = ".txt")
# filenames <- purrr::flatten(text_files)
# Initialize an empty list to store the data
# speech_data <- list()
speech_data <- c()
i = 0
num_chars <- c(27050, 12786, 39019, 39524, 37489, 45247, 34674, 41225, 37552, 41719, 50544, 58284, 34590, 39232, 54635, 48643, 48641, 44907, 31101, 47157, 26384, 33281, 33376, 36006, 29403, 36233, 32860, 32464, 35981, 33290, 42112, 56960, 47910, 43352, 52972, 60000)
# Loop through the list of text files and read them into R
for (file in text_files) {
  i = i + 1
  # speech <- readLines(file, warn = FALSE)
  file_handle <- file(paste("data/", file, sep = ""), "r")
  speech <- readChar(file_handle, nchars = num_chars[i])
  # speech_data[[file]] <- speech
  speech_data[i] <- speech
  close(file_handle)
}

sona <- data.frame(filename = text_files, speech = speech_data, stringsAsFactors = FALSE)

# extract year and president for each speech
sona$year <- str_sub(sona$filename, start = 1, end = 4)
sona$president <- str_remove_all(str_extract(sona$filename, "[dA-Z].*\\."), "\\.")

# clean the sona dataset by adding the date and removing unnecessary text
replace_reg <- '(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\n'
unnest_reg <- "[^A-Za-z_\\d#@']"
sona <-sona %>%
  mutate(speech = str_replace_all(speech, replace_reg , ' ')
         ,date = str_sub(speech, start=1, end=30)
         ,date = str_replace_all(date, "February", "02")
         ,date = str_replace_all(date, "June", "06")
         ,date = str_replace_all(date, "Feb", "02")
         ,date = str_replace_all(date, "May", "05")
         ,date = str_replace_all(date, "Jun", "06")
         ,date = str_replace_all(date, "Thursday, ","")
         ,date = str_replace_all(date, ' ', '-')        
         ,date = str_replace_all(date, "[A-z]",'')
         ,date = str_replace_all(date, '-----', '')
         ,date = str_replace_all(date, '----', '')
         ,date = str_replace_all(date, '---', '')
         ,date = str_replace_all(date, '--', '')
  )

sona$date[36] <- "09-02-2023"
sona$year[36] <- "2023"
sona$date <- dmy(sona$date)
```

```{r}
#| label: preprocess

speech_tokens <- sona %>%
  unnest_tokens(word, speech, token = "regex", pattern = unnest_reg) %>%
  anti_join(stop_words)
  
words_to_remove <- c("government", "South Africa", "national",
                     "country", "south", "africa", "honourable", 
                     "people")

speech_tokens <- speech_tokens %>%
  filter(!word %in% words_to_remove)


load("dsfi-lexicons.Rdata")
```

## Sentiment Analysis

The aim of the sentiment analysis is to determine the emotional tonality expressed in the State of the Nation Address. At the core of this sentiment analysis lies the sentiment lexicon---a dictionary that associates words or phrases with sentiment scores. In this paper, three different lexicons found in the 'tidyr' package were used: AFINN, bing and nrc. The AFINN lexicon assigns a score to each word in the topic with -5 being the most negative and 5 being the most postive score.The bing lexicon assigns each word in the tolkenized speeches as either negative or positive according to the lexicon association. The nrc lexicon assigns each word in the tolkenized speeches into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

## Latent Dirichlet Allocation

LDA is, by its nature, an iterative process. First, determining the number of K's (topics) is an important first step in topic modelling. This leads to the organization and determination of our topics. The interpretation of topics largely depends on the actual context of the speeches and the choice of words. The coherence and relevance of topics needs to be established in order to make meaning out of the model's results.

In addition to the data preprocessing steps for sentiment analysis, particular care was taken to ensure that repetition across topics was minimized as much as possible given the number of topics which we decided to focus on (expanded on further below). Overlap between topics is normal. However, given the number of decided topics, the distinctiveness and interpretability of the topics was initially difficult to distinguish. This is especially evident in a large corpus of text like in our dataset. Thus less informative or repetitive words were removed.

To perform topic modeling, we first created a document-term matrix (DTM), which is a sparse matrix containing the terms and documents in our corpus. We then removed terms that appeared fewer than two times or in more than half of the documents in order to improve computer processing. Next, we ran the LDA algorithm on the DTM to identify the latent topics in the collection of texts.

Deciding on the optimal number of topics was done by training the LDA model for each number of topics from two to ten and computing the average of the coherence scores for each topic in each model. Coherence scores are a measure of how well the topics in the model explain the data [@blei2003lda]. It computes the pairwise similarity between the top words in each topic by giving a probabilistic coherence score for each topic. Semantically coherent topics are indicated by higher coherence scores. Stated differently, the higher the score for the specified number of K; for each topic, there will be more related words together and the topic would make more sense. Our dataset has speeches spanning nearly thirty years for six different presidents: thus, despite a lower value of K being associated with higher coherence, we could possibly lose a lot of nuance by choosing a lower value. Although, choosing a higher value of K could lead to overfitting and a higher chance of overlap or redundancy between topics [@blei2009].

```{r}
#| label: fig-ave-coherence
#| fig-cap: "Topic Coherence Scores (Averaged). Higer values indicates that number of topics are more meaningful (in comparison)" 

sona_tidy <- sona %>% 
  unnest_tokens(word, speech, token = "words", to_lower = T) %>%
  filter(!word %in% stop_words$word) %>%
  filter(!str_detect(word, "[0-9]"))

speech_tdf <- sona_tidy %>%
  group_by(date,word) %>%
  count() %>%  
  ungroup()

dtm_speech <- speech_tdf %>% 
  cast_dtm(date, word, n)

# Step 1: Remove terms appearing in more than half of the documents
dtm_speech_filtered <- removeSparseTerms(dtm_speech, sparse = 0.5)  # Keep terms that appear in less than half of the documents

# Step 2: Manually remove terms that appear less than 2 times
dtm_speech_filtered <- removeSparseTerms(dtm_speech_filtered, sparse = 0.99)

# Evaluate a range of topic numbers from 2 to 10
k_list <- seq(2, 10, by = 1)

# Train LDA models for each topic number
models <- lapply(k_list, function(k) LDA(dtm_speech_filtered, k = k, control = list(seed = 2023)))

# Calculate the topic coherence score for each model
coherence <- sapply(models, function(model) topic_coherence(model, dtm_speech_filtered))

average_coherence_scores <- c()

for (i in seq_along(coherence)) {
  average_coherence_scores[i] <- mean(coherence[[i]])
}

# Plot the topic coherence scores
ggplot(data.frame(k = k_list, coherence = average_coherence_scores), aes(x = k, y = coherence)) +
  geom_point() +
  geom_line(group = 1) +
  ggtitle("Topic Coherence Scores") +
  xlab("Number of Topics") +
  ylab("Coherence Score")
```

We plotted the averaged coherent scores for each value of K and found that the highest coherence score was achieved at ***k*** = 2 (followed by 4, then 3) as shown in @fig-ave-coherence. These are

The three factors of interpretability, distinctiveness, and relevance were also taken into account in addition to the averaged coherence scores for each value of K. Each of the three factors will be expanded on below:

-   ***Interpretability***: A smaller number of topics is generally more interpretative, as it easier to understand a smaller number of more general topics. For a larger number of topics, the topics may be more specific and less distinct [@blei2009].

-   ***Distinctiveness***: A larger number of topics is generally more distinctive, as the topics are more specialized and less likely to overlap [@blei2009].

-   ***Word relevance***: In a large number of topics, each word will be associated with a narrower range of topics [@blei2009].

Given the trade-off between these factors as well as the coherence score of each number of topics, seven was decided on as the optimal amount of topics (even though two, four, and three had higher coherence scores). Relative interpretability and coherence was sacrificed for more distinctiveness and word relevance.

Several words, such as *african*, *include*, *programme* appeared in most of the topics after the algorithm was run. These words either didn't provide much meaning to the topic, or were making the interpretation of the topic more difficult, so they were removed.

# Results

## Sentiment Analysis

```{r}
#| label: bing sentiment

speech_sentiment <- speech_tokens %>% 
  left_join(bing, by = "word") %>%
  rename(bing_sentiment = sentiment) %>%
  mutate(bing_sentiment = ifelse(is.na(bing_sentiment), 'neutral', bing_sentiment))

```

```{r}
#| label: fig-positive
#| fig-cap: "Most positive words used by each president"

speech_sentiment %>%
  filter(bing_sentiment == 'positive') %>%
  count(president, word) %>%
  group_by(president) %>% filter(rank(desc(n)) <= 5) %>%
  ggplot(aes(reorder(word,n),n)) + geom_col(aes(fill = word)) + facet_wrap(~president)+
  coord_flip() + labs( y = 'Contribution to Sentiment')
```

```{r}
#| label: fig-nwords
#| fig-cap: "Most Negative words used by each president"
speech_sentiment %>%
  filter(bing_sentiment == 'negative') %>%
  count(president, word) %>%
  group_by(president) %>% filter(rank(desc(n)) <= 5) %>%
  ggplot(aes(reorder(word,n),n)) + geom_col() + facet_wrap(~president) + coord_flip() + xlab('')

```

```{r}
#| label: nrc

speech_sentiment <- speech_sentiment %>% 
left_join(nrc, by = "word") %>%
rename(nrc_sentiment = sentiment)

speech_sentiment%>% 
  count(president, word) %>%
  group_by(president) %>% filter(rank(desc(n)) <= 5) %>%
  ggplot(aes(reorder(word,n),n)) + geom_col(aes(fill =word)) + facet_wrap(~president) + coord_flip() + labs( y = 'Contribution to Sentiment')


```

We can see from @fig-positive that positive words such as 'reconciliation', 'freedom', 'progress', and 'improve' were commonly occurring in speeches made by Nelson Mandela. The word occurence of 'support' in the speeches seems to be increasing as we move towards the most recent president, Cyril Ramaphosa.

@fig-nwords shows that most negative words used in each of the presidents. This may highlight some of the issues each president had experienced during their tenure. 'Crime' seems to be a word with one of the most commonly occurring in speeches made by Nelson Mandela, Thabo Mbeki, and Jacob Zuma. 'Poverty' is the most occuring negative word in Thabo Mbeki's speeches and also significant in Jacob Zuma's, and Cyril Ramaphosa's speeches.

```{r}
#| label: fig-overall
#| fig-cap: "Proportion of sentiment words in speeches for each president"

speech_sentiment%>%
  add_count(president,date, name = "n_words") %>%
  na.omit() %>%
  group_by(president,date, nrc_sentiment) %>%
  summarize(mean_prop = n() / first(n_words)) %>% ungroup() %>%
  ggplot(aes(reorder(nrc_sentiment, mean_prop), mean_prop, fill = president)) + 
  geom_bar(stat = "identity", position = 'dodge') + coord_flip() + xlab('')
```

@fig-overall indicates the proportion of sentiment words, categorized using the nrc lexicon, for each of the presidents. Nelson Mandela's speeches have the highest proportion of positive sentiment while De Klerk's speeches have the the highest negative sentiment. Nelson Mandela's speeches have a relatively high proportion of words associated with anticipation, and fear, but also of joy and trust.

```{r}
#| label: fig-sentimentscore
#| fig-cap: "Time series Plot of Average Sentiment using AFINN Lexicon Scoring System. Higher Values Represent more Positive Sentiment" 

speech_sentiment <- speech_sentiment %>% 
  left_join(afinn, by = "word") %>%
  rename(afinn_value = value) 

speech_sentiment%>%
  select(date, president, word, afinn_value)%>%
  na.omit%>% 
  group_by( date, president) %>%
  summarise(.groups = "drop", Mean_Score = mean(afinn_value)) %>%
  ungroup() %>%
  ggplot(aes(x = date, y = Mean_Score, col = president))+ geom_line() +
  labs(title = "Average Sentiment Score Time Series", x = "date", y = " Average Sentiment Score") +
  theme_minimal()
```

@fig-sentimentscore shows a time series plot provides an overview of how sentiments have evolved over the years in the SONA speeches. There is a major spike in postive sentiment in Thabo Mbeke's address in May 2004 right after his re-election on 14th April 2004. This is followed by a lower average sentiment score for the rest of Mbeki's State of the Nation Addresses for the his second tenure. There was also a sharp decline in Jacob Zuma's SONA on 14th February 2003. Since Ramaphosa's time in office there has been an overall decline in the average sentiment score of the SONA each with the address in February 2022 having the lowest sentiment score since 1994.

## Latent Dirichlet Allocation

```{r}
#| label: LDA preprocessing

words_to_remove <- c("government", "South Africa", "national",
                     "country", "south", "africa", "honourable", 
                     "people", 'ensure', 'public', 'continue', 'regard', 'development', 'support', 'africans', 'african', 'programme', 'programmes',  'compatriots', 'including',  'improve', 'address', 'president', 'deputy', 'services', 'chairperson', 'speaker', 'madame', 'sector', 'social', 'system', 'service', 'growth', 'million', 'past', 'time', 'process', 'world', 'progress', 'economy', 'economic', 'cape', 'parties', 'ago', 'set', 'matter', 'manner')

tidy_speeches <- sona %>% 
  unnest_tokens(word, speech, token = "words", to_lower = T) %>%
  filter(!word %in% stop_words$word) %>%
  filter(!word %in% words_to_remove) %>%
  filter(!str_detect(word, "[0-9]"))

speech_tdf <- tidy_speeches%>%
  group_by(date,word) %>%
  count() %>%  
  ungroup()

dtm_speech <- speech_tdf %>% 
  cast_dtm(date, word, n)

# Step 1: Remove terms appearing in more than half of the documents
dtm_speech_filtered <- removeSparseTerms(dtm_speech, sparse = 0.5)  # Keep terms that appear in less than half of the documents

# Step 2: Manually remove terms that appear once
dtm_speech_filtered <- removeSparseTerms(dtm_speech_filtered, sparse = 0.99)
```

```{r}
#| label: LDA model

speech_lda <- LDA(dtm_speech_filtered, k = 7, control = list(seed = 2023))

speech_topics <- tidy(speech_lda, matrix = 'beta')
```

```{r}
#| label: fig-topterms-lda
#| fig-cap: "Word-Topic Associations"  

top_terms <- speech_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

@fig-topterms-lda shows the most frequent words for each of the eight topics organized in descending order of the words' per-topic-per-word probabilities. The topics can be interpreted as follows:

1.  **Economic Development and Job Creation**: this category represents topics related to economic development, job creation, and infrastructure development to support economic growth.
2.  **Infrastructure and Energy Planning**: this category centers on infrastructure and energy planning, possibly related to financial and local development.
3.  **Constitutional and Democratic Governance**: this category revolves around constitutional governance, democratic values, and human resources within a society.
4.  **Public Safety and Community**: this category relates to issues of public safety, democracy, community life, and national security.
5.  **Poverty Alleviation and Local Development**: this category emphasizes poverty allevation, improving security, and local development, with a focus on implementation and leadership.
6.  **Infrastructure and Economic Opportunities:** this category covers infrastructure development, education, and economic opportunities, possibly with a focus on specific projects and inclusivity.
7.  **International Engagement and Human Development**: this category delves into discussions related to international engagement, human development, poverty reduction, and the legacy of figures like Nelson Mandela.

```{r}
#| label: speech_gamma

sona$speechId <- as.numeric(speech_lda@documents)

speeches_gamma_init <- tidy(speech_lda, matrix = "gamma")

speeches_gamma <- sona %>% 
    left_join(speeches_gamma_init %>% 
    mutate(speechId = as.numeric(document)) %>% 
    select(-document) %>%
    spread(key = topic, value = gamma, sep = "_"))
```

```{r}
#| label: fig-prez-topic
#| fig-cap: "President-Topic Association Boxplot"

# Reshape the data to long format
speeches_long <- speeches_gamma %>%
  select(year, president, starts_with("topic")) %>%
  pivot_longer(cols = starts_with("topic"), names_to = "topic", values_to = "gamma")

# Create a boxplot of each president's influence on each topic
ggplot(speeches_long, aes(x = president, y = gamma, fill = topic)) +
  geom_boxplot() +
  labs(x = "President", y = "Topic Influence (Gamma Score)", fill = "Topic") +
  scale_fill_discrete(name = "Topic") +
  theme_minimal()
```

@fig-prez-topic shows the *per-document-per-topic* probability by gamma. It shows how well the LDA model distinguishes between the different topics for each president. President de Klerk and President Motlanthe's distributions are noteworthy here: each is only represented by one speech, hence not much relevance is established to the topics. The dataset is unbalanced in this regard. de Klerk was the outgoing president at the turn of democracy in 1994, and Motlanthe served as the interim president when President Mbeki was recalled by his own party, the African National Congress (ANC), for the remainder of Mbeki's term.

Certain topics are associated with certain presidents as time passes. President Mandela's term heralded the birth of modern South Africa. His speeches have notable associations with nation building and democracy (topic 1) and establishing a new society (topic 5) [@sagov].

President Mbeki's, in turn, spoke to addressing poverty, increasing economic prosperity (Broad-Based Black Economic Empowerment legislation came into effect in 2003), strengthening international ties [@sagov].

President Zuma focussed quite heavily on infrastructure and economic growth. Indeed, he inherited the issue of Eskom's, the country's largest producer of electricity, energy crisis as well as foresaw the 2010 FIFA World Cup under his presidency [@sagov].

Lastly, President Ramaphosa has an association with continued infrastructure development, business investment and job growth (he is noted as an adept businessman). The model, though, didn't pick up on the health concerns (less than a year after his presidential term began, the COVID-19 virus caused a global pandemic) and anti-corruption rhetoric in his speeches: he succeeded President Zuma, who resigned in 2018 amidst his misuse of state expenditure for personal gain. His term is also associated with advocating for peace amidst the gender-based violence protests that occurred towards the end of 2019, which the model failed to highlight [@sagov].

```{r}
#| label: fig-topic-diagnostics
#| fig.cap: "Topic Diagnostics"

diag_df <- topic_diagnostics(speech_lda, dtm_speech_filtered)

diag_df <- diag_df %>%
  mutate(topic_label = terms(speech_lda, 5) %>%
           apply(2, paste, collapse = ", "),
         topic_label = paste(topic_num, topic_label, sep = " - "))

diag_df %>% 
  gather(diagnostic, value, -topic_label, -topic_num) %>%
  ggplot(aes(x = topic_num, y = value,
             fill = str_wrap(topic_label, 25))) +
  geom_bar(stat = "identity") +
  facet_wrap(~diagnostic, scales = "free") +
  labs(x = "Topic Number", y = "Diagnostic Value",
       fill = "Topic Label", title = "All Topic Model Diagnostics")
```

@fig-topic-diagnostics shows how the model performed in various metrics. Of particular note are the ***dist_from_corpus***, ***doc_prominence***, ***topic_coherence (which we touched on earlier)***, and ***topic_exclusivity*** plots.

The *distance from the corpus* is a measure of how different a document is from the rest of the corpus by calculating the average distance between a topic's distribution and the topic distributions of all other documents in the collection. Higher scores indicate difference in the corpus [@angelov2020].

*Document prominence* is a measure of how important a document is to a particular topic, by calculating the weighted average of the document's topic distribution (probabilities). Higher scores indicate importance to the corpus [@angelov2020].

Lastly, *topic exclusivity* is a measure of how specific a topic is to a particular document, by calculating the normalized average of the topic's distribution. Higher scores here indicate the specificity of the topic to the document [@angelov2020].

We note that Topic 3, which we labelled as ***Constitutional and Democratic Governance***, scores lowly in two of these metrics: it has little importance to the the corpus, is the most notably incoherent, and seems to be an outlier to the rest of the topics.

The coherence scores here do leave much to be desired: perhaps the model would have to undergo additional hyperparameter tuning to mitigate the relatively low coherence scores.

```{r}
#| label: fig-topic-time
#| fig-cap: "Topic Relevance Over Time"
#| fig-width: 10
#| fig-height: 10

# Reshape the data to long format for plotting
speeches_long <- speeches_gamma %>%
  gather(topic, proportion, starts_with("topic")) %>%
  mutate(topic = sub("topic_", "", topic) %>% as.numeric()) %>%
  filter(proportion > 0.5) %>%
  arrange(year, president)

all_years <- all_years <- unique(speeches_long$year)

# speeches_long['topic'][speeches_long['topic'] == 1] <- '1: National Aspirations & Democracy'
# speeches_long['topic'][speeches_long['topic'] == 2] <- '2: Security, Jobs, & Local Development'
# speeches_long['topic'][speeches_long['topic'] == 3] <- '3: Societal Implementation & Poverty Alleviation'
# speeches_long['topic'][speeches_long['topic'] == 4] <- '4: Infrastructure, Business, and Education'
# speeches_long['topic'][speeches_long['topic'] == 5] <- '5: Business Investment & Health'
# speeches_long['topic'][speeches_long['topic'] == 6] <- '6: Society, Security, & Local Governance'
# speeches_long['topic'][speeches_long['topic'] == 7] <- '7: Human Welfare, Peace, & Global Engagement'
# speeches_long['topic'][speeches_long['topic'] == 8] <- '8: Infrastructure Investment & Anti-Corruption'

# Create a stacked bar plot
ggplot(speeches_long, aes(x = factor(year, levels = all_years), fill = factor(topic))) +
  geom_bar() +
  labs(x = "Year", y = "Number of Speeches", fill = "Topic") +
  scale_fill_discrete(name = "Topic") +
  theme_minimal() +
  scale_x_discrete(expand = c(0, 0))
```

@fig-topic-time shows the relevance of each topic over time. This graph can be interpreted along with @fig-prez-topic, seeing as, respectively, each president term-of-office was as follows:

-   de Klerk: 1989 - 1994

-   Mandela: 1994 - 1999

-   Mbeki: 1999 - 2008

-   Motlanthe - 2008-2009

-   Zuma: 2009 - 2018

-   Ramaphosa: 2018 - current [@sagov]

Each presidential administration faces unique challenges that distinguishes itself from the other; and each presidential term is five years, which coincides with the election cycle. Thus the theme-presidential association applies here, too. It is notable though that increased focus has been spent on economic and infrastructural development as South Africa experienced worsening economic indicators (weak GDP growth, high unemployment) and increased rolling blackouts in recent years (2018 - current) [@sagov]. These point to the weakening of South Africa's public institutions - a sign of the malaise that has crept into the ruling party.

# Discussion & Conclusion

The SONA serves as a significant pillar of democratic governance, whereby the President of South Africa addresses Parliament, as established earlier. The existing literature touts data mining techniques such as sentiment analysis and LDA as valuable tools in helping analyze political speeches. They each present a framework for facilitating a deeper understanding of the content and themes of across a large body of text. The integration of sentiment analysis was used to uncover the emotional tonality, while LDA identifies the thematic topics present in these significant political addresses.

In our research, we obtained the full text of SONA speeches from the official South African Government website, spanning nearly three decades and six different presidents. The preprocessing steps encompassed identifying key information, removing noise-inducing elements, tokenization, and eliminating stop words. These steps collectively enhanced the quality and focus of the analysis, allowing for a more insightful understanding of the content and themes of the SONA speeches.

The use of diverse sentiment lexicons provides a comprehensive insight into the emotional nuances present in the speeches.

The iterative nature of LDA is essential to determine the number of topics, organize these topics, and interpret them. However, the challenge of coherence and relevance must be managed, especially in a large corpus like the SONA speeches dataset. Steps were taken to minimize word repetition and enhance the distinctiveness and interpretability of the topics.

The selection of the optimal number of topics was achieved through the assessment of coherence scores, striking a balance between semantic coherence and the risk of overfitting in a dataset.

While the sentiment analysis proved effective, our model didn't perform as well as expected on this particular dataset. It is worth considering hyperparameter tuning for better results.

The lessons gained from this study, by combining sentiment analysis and LDA, can equip us to conduct a nuanced and rigorous analysis of the SONA speeches, furthering our understanding of their emotional content and thematic patterns in political discourse.

\newpage

### References

::: {#refs}
:::
