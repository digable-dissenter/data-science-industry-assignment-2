knitr::opts_knit$set(root.dir = "C:/Users/User/OneDrive/Documents/School/2023/Masters/STA5073Z/Assignments/Assignment 2/ds4l-assignment-2/data/")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, fig.pos = "H", out.extra = "")
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'text2vec', 'tidyr', 'tidytext', 'topicmodels', 'tm', 'wordcloud')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
# Get a list of all text files in the directory
text_files <- list.files(pattern = ".txt")
# Initialize an empty list to store the data
speech_data <- list()
# Loop through the list of text files and read them into R
for (file in text_files) {
speech <- readLines(file, warn = FALSE)
speech_data[[file]] <- speech
}
#data wrangling
speech_tibble <- tibble(speech = speech_data)
speech_tibble <- speech_tibble %>%
separate(speech, into = c("day", "month" , "year","content"), sep = " ",fill= "right", extra = "merge")
speech_tibble <- speech_tibble %>%
mutate(
content = tolower(content),
content = gsub("[^[:alnum:]']", " ", content),
day = gsub("[c(,'’\"]", "", day),
year = gsub("[,'’\"]", "", year)
) %>% unite("Date", day:month:year, remove = TRUE, sep = " ")
speech_tibble$Date[35]<- "10 February 2022"
speech_tibble$Date[36]<- "9 February 2022"
speech_tibble$Date<- dmy(speech_tibble$Date)
# Assuming you have already loaded the 'lubridate' package
speech_tibble <- speech_tibble %>%
mutate(
President = case_when(
Date == ymd("1994-02-28") ~ "F.W. de Klerk",
Date > ymd("1994-02-28") & Date < ymd("1999-06-14") ~ "Nelson Mandela",
Date >= ymd("1999-06-14") & Date < ymd("2008-09-24") ~ "Thabo Mbeki",
Date >= ymd("2008-09-24") & Date <= ymd("2009-05-09") ~ "Kgalema Motlanthe",
Date >= ymd("2009-05-09") & Date <= ymd("2018-02-14") ~ "Jacob Zuma",
TRUE ~ "Cyril Ramaphosa"
)
)
# Create a custom preprocessing function
preprocess_text <- function(text) {
# Preprocess the text
text <- tolower(text)
text <- gsub("[^[:alnum:]']", " ", text)
# Separate text into words
words <- unlist(strsplit(text, "\\s+"))
# Remove stop words
words <- words[!words %in% stopwords("en")]
# Join the cleaned words back into a single string
cleaned_text <- paste(words, collapse = " ")
# Return the cleaned text
return(cleaned_text)
}
# Apply the custom preprocessing function to each speech
speech_tibble <- speech_tibble %>%
mutate(content = map_chr(content, preprocess_text))
??map_chr
knitr::opts_knit$set(root.dir = "C:/Users/User/OneDrive/Documents/School/2023/Masters/STA5073Z/Assignments/Assignment 2/ds4l-assignment-2/data/")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, fig.pos = "H", out.extra = "")
install.packages("shiny")
install.packages("learnr")
knitr::opts_knit$set(root.dir = "C:/Users/User/OneDrive/Documents/School/2023/Masters/STA5073Z/Assignments/Assignment 2/ds4l-assignment-2/data/")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, fig.pos = "H", out.extra = "")
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'text2vec', 'tidyr', 'tidytext', 'topicmodels', 'tm', 'wordcloud')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
unzip("sona-addresses-1994-2023.zip")
# Get a list of all text files in the directory
text_files <- list.files(pattern = ".txt")
# Initialize an empty list to store the data
speech_data <- list()
# Loop through the list of text files and read them into R
for (file in text_files) {
speech <- readLines(file, warn = FALSE)
speech_data[[file]] <- speech
}
#data wrangling
speech_tibble <- tibble(speech = speech_data)
speech_tibble <- speech_tibble %>%
separate(speech, into = c("day", "month" , "year","content"), sep = " ",fill= "right", extra = "merge")
speech_tibble <- speech_tibble %>%
mutate(
content = tolower(content),
content = gsub("[^[:alnum:]']", " ", content),
day = gsub("[c(,'’\"]", "", day),
year = gsub("[,'’\"]", "", year)
) %>% unite("Date", day:month:year, remove = TRUE, sep = " ")
speech_tibble$Date[35]<- "10 February 2022"
speech_tibble$Date[36]<- "9 February 2022"
speech_tibble$Date<- dmy(speech_tibble$Date)
# Assuming you have already loaded the 'lubridate' package
speech_tibble <- speech_tibble %>%
mutate(
President = case_when(
Date == ymd("1994-02-28") ~ "F.W. de Klerk",
Date > ymd("1994-02-28") & Date < ymd("1999-06-14") ~ "Nelson Mandela",
Date >= ymd("1999-06-14") & Date < ymd("2008-09-24") ~ "Thabo Mbeki",
Date >= ymd("2008-09-24") & Date <= ymd("2009-05-09") ~ "Kgalema Motlanthe",
Date >= ymd("2009-05-09") & Date <= ymd("2018-02-14") ~ "Jacob Zuma",
TRUE ~ "Cyril Ramaphosa"
)
)
# Create a custom preprocessing function
preprocess_text <- function(text) {
# Preprocess the text
text <- tolower(text)
text <- gsub("[^[:alnum:]']", " ", text)
# Separate text into words
words <- unlist(strsplit(text, "\\s+"))
# Remove stop words
words <- words[!words %in% stopwords("en")]
# Join the cleaned words back into a single string
cleaned_text <- paste(words, collapse = " ")
# Return the cleaned text
return(cleaned_text)
}
# Apply the custom preprocessing function to each speech
speech_tibble <- speech_tibble %>%
mutate(content = map_chr(content, preprocess_text))
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'purrr', 'text2vec', 'tidyr', 'tidytext', 'topicmodels', 'tm', 'wordcloud')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
# Get a list of all text files in the directory
text_files <- list.files(pattern = ".txt")
# Initialize an empty list to store the data
speech_data <- list()
# Loop through the list of text files and read them into R
for (file in text_files) {
speech <- readLines(file, warn = FALSE)
speech_data[[file]] <- speech
}
#data wrangling
speech_tibble <- tibble(speech = speech_data)
speech_tibble <- speech_tibble %>%
separate(speech, into = c("day", "month" , "year","content"), sep = " ",fill= "right", extra = "merge")
speech_tibble <- speech_tibble %>%
mutate(
content = tolower(content),
content = gsub("[^[:alnum:]']", " ", content),
day = gsub("[c(,'’\"]", "", day),
year = gsub("[,'’\"]", "", year)
) %>% unite("Date", day:month:year, remove = TRUE, sep = " ")
speech_tibble$Date[35]<- "10 February 2022"
speech_tibble$Date[36]<- "9 February 2022"
speech_tibble$Date<- dmy(speech_tibble$Date)
# Assuming you have already loaded the 'lubridate' package
speech_tibble <- speech_tibble %>%
mutate(
President = case_when(
Date == ymd("1994-02-28") ~ "F.W. de Klerk",
Date > ymd("1994-02-28") & Date < ymd("1999-06-14") ~ "Nelson Mandela",
Date >= ymd("1999-06-14") & Date < ymd("2008-09-24") ~ "Thabo Mbeki",
Date >= ymd("2008-09-24") & Date <= ymd("2009-05-09") ~ "Kgalema Motlanthe",
Date >= ymd("2009-05-09") & Date <= ymd("2018-02-14") ~ "Jacob Zuma",
TRUE ~ "Cyril Ramaphosa"
)
)
# Create a custom preprocessing function
preprocess_text <- function(text) {
# Preprocess the text
text <- tolower(text)
text <- gsub("[^[:alnum:]']", " ", text)
# Separate text into words
words <- unlist(strsplit(text, "\\s+"))
# Remove stop words
words <- words[!words %in% stopwords("en")]
# Join the cleaned words back into a single string
cleaned_text <- paste(words, collapse = " ")
# Return the cleaned text
return(cleaned_text)
}
# Apply the custom preprocessing function to each speech
speech_tibble <- speech_tibble %>%
mutate(content = map_chr(content, preprocess_text))
# Create a corpus from your data
corpus <- Corpus(VectorSource(speech_tibble$content))
# Continue with your existing preprocessing steps
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Now you can proceed with LDA or any other analysis using 'dtm'
View(speech_tibble)
lda_model <- LDA(dtm, k = 2, control = list(seed = 1234))
# Get the top words for each topic
top_words_per_topic <- terms(lda_model, 10)
top_words_per_topic
#data wrangling
speech_tibble <- tibble(speech = speech_data)
speech_tibble <- speech_tibble %>%
separate(speech, into = c("day", "month" , "year","content"), sep = " ",fill= "right", extra = "merge")
speech_tibble <- speech_tibble %>%
mutate(
content = tolower(content),
content = gsub("[^[:alnum:]']", " ", content),
day = gsub("[c(,'’\"]", "", day),
year = gsub("[,'’\"]", "", year)
) %>% unite("Date", day:month:year, remove = TRUE, sep = " ")
speech_tibble$Date[35]<- "10 February 2022"
speech_tibble$Date[36]<- "9 February 2022"
speech_tibble$Date<- dmy(speech_tibble$Date)
# Assuming you have already loaded the 'lubridate' package
speech_tibble <- speech_tibble %>%
mutate(
President = case_when(
Date == ymd("1994-02-28") ~ "F.W. de Klerk",
Date > ymd("1994-02-28") & Date < ymd("1999-06-14") ~ "Nelson Mandela",
Date >= ymd("1999-06-14") & Date < ymd("2008-09-24") ~ "Thabo Mbeki",
Date >= ymd("2008-09-24") & Date <= ymd("2009-05-09") ~ "Kgalema Motlanthe",
Date >= ymd("2009-05-09") & Date <= ymd("2018-02-14") ~ "Jacob Zuma",
TRUE ~ "Cyril Ramaphosa"
)
)
speech_tokens <- speech_tibble %>%
unnest_tokens(word, content) %>%
anti_join(stop_words)
load("dsfi-lexicons.Rdata")
speech_tdf <- speech_tokens%>%
group_by(Date,word) %>%
count() %>%
ungroup()
dtm_speech <- speech_tdf %>%
cast_dtm(Date, word, n)
lda_model <- LDA(dtm_speech, k = 2, control = list(seed = 1234))
# Get the top words for each topic
top_words_per_topic <- terms(lda_model, 10)
top_words_per_topic
lda_model <- LDA(dtm_speech, k = 5, control = list(seed = 1234))
# Get the top words for each topic
top_words_per_topic <- terms(lda_model, 10)
top_words_per_topic
# Extract the document-topic distribution
document_topics <- tidy(lda_model, matrix = "beta")
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'purrr', 'reshape2', 'text2vec', 'tidyr', 'tidytext', 'topicmodels', 'tm', 'wordcloud')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
# Get a list of all text files in the directory
text_files <- list.files(pattern = ".txt")
# Initialize an empty list to store the data
speech_data <- list()
# Loop through the list of text files and read them into R
for (file in text_files) {
speech <- readLines(file, warn = FALSE)
speech_data[[file]] <- speech
}
#data wrangling
speech_tibble <- tibble(speech = speech_data)
speech_tibble <- speech_tibble %>%
separate(speech, into = c("day", "month" , "year","content"), sep = " ",fill= "right", extra = "merge")
speech_tibble <- speech_tibble %>%
mutate(
content = tolower(content),
content = gsub("[^[:alnum:]']", " ", content),
day = gsub("[c(,'’\"]", "", day),
year = gsub("[,'’\"]", "", year)
) %>% unite("Date", day:month:year, remove = TRUE, sep = " ")
speech_tibble$Date[35]<- "10 February 2022"
speech_tibble$Date[36]<- "9 February 2022"
speech_tibble$Date<- dmy(speech_tibble$Date)
# Assuming you have already loaded the 'lubridate' package
speech_tibble <- speech_tibble %>%
mutate(
President = case_when(
Date == ymd("1994-02-28") ~ "F.W. de Klerk",
Date > ymd("1994-02-28") & Date < ymd("1999-06-14") ~ "Nelson Mandela",
Date >= ymd("1999-06-14") & Date < ymd("2008-09-24") ~ "Thabo Mbeki",
Date >= ymd("2008-09-24") & Date <= ymd("2009-05-09") ~ "Kgalema Motlanthe",
Date >= ymd("2009-05-09") & Date <= ymd("2018-02-14") ~ "Jacob Zuma",
TRUE ~ "Cyril Ramaphosa"
)
)
speech_tokens <- speech_tibble %>%
unnest_tokens(word, content) %>%
anti_join(stop_words)
load("dsfi-lexicons.Rdata")
speech_sentiment <- speech_tokens %>%
left_join(bing, by = "word") %>%
rename(bing_sentiment = sentiment) %>%
mutate(bing_sentiment = ifelse(is.na(bing_sentiment), 'neutral', bing_sentiment))
#5 most positive words for each movie
speech_sentiment %>%
filter(bing_sentiment == 'positive') %>%
count(President, word) %>%
group_by(President) %>% filter(rank(desc(n)) <= 5) %>%
ggplot(aes(reorder(word,n),n)) + geom_col() + facet_wrap(~President)+
coord_flip() + xlab('')
#5 most negative words for each movie
speech_sentiment %>%
filter(bing_sentiment == 'negative') %>%
count(President, word) %>%
group_by(President) %>% filter(rank(desc(n)) <= 5) %>%
ggplot(aes(reorder(word,n),n)) + geom_col() + facet_wrap(~President) + coord_flip() + xlab('')
speech_sentiment <- speech_sentiment %>%
left_join(nrc, by = "word") %>%
rename(nrc_sentiment = sentiment)
speech_sentiment%>%
add_count(President,Date, name = "n_words") %>%
na.omit() %>%
group_by(President,Date, nrc_sentiment) %>%
summarize(prop = n() / first(n_words)) %>% ungroup() %>%
group_by(President, nrc_sentiment) %>%
summarize(mean_prop = mean(prop)) %>% ungroup() %>%
ggplot(aes(reorder(nrc_sentiment, mean_prop), mean_prop, fill = President)) +
geom_bar(stat = "identity", position = 'dodge') + coord_flip() + xlab('')
speech_tdf <- speech_tokens%>%
group_by(Date,word) %>%
count() %>%
ungroup()
dtm_speech <- speech_tdf %>%
cast_dtm(Date, word, n)
lda_model <- LDA(dtm_speech, k = 5, control = list(seed = 1234))
# Get the top words for each topic
top_words_per_topic <- terms(lda_model, 10)
# Extract the document-topic distribution
document_topics <- tidy(lda_model, matrix = "beta")
# Assuming you have a data frame containing your speeches, add the document-topic distribution to it
speech_data_with_topics <- cbind(speech_data, document_topics)
speech_tdf <- speech_tokens%>%
group_by(Date,word) %>%
count() %>%
ungroup()
dtm_speech <- speech_tdf %>%
cast_dtm(Date, word, n)
lda_model <- LDA(dtm_speech, k = 2, control = list(seed = 1234))
# Get the top words for each topic
top_words_per_topic <- terms(lda_model, 10)
# Extract the document-topic distribution
document_topics <- tidy(lda_model, matrix = "beta")
# Assuming you have a data frame containing your speeches, add the document-topic distribution to it
speech_data_with_topics <- cbind(speech_data, document_topics)
# Calculate coherence scores for topics
coherence_scores <- topic_model_coherence(speech_lda, dtm_speech)
??topic_model_coherence
# Assuming you have a data frame containing your speeches (speech_data)
# Extract the "document" column (or another suitable identifier) from speech_data
speech_data$document <- 1:nrow(speech_data)  # Assuming you have an identifier
# Extract the document-topic distribution
document_topics <- tidy(lda_model, matrix = "beta")
# Assuming you have a data frame containing your speeches (speech_data)
# Extract the "document" column (or another suitable identifier) from speech_data
speech_data$document <- 1:nrow(speech_data)  # Assuming you have an identifier
speech_data
# Assuming you have speech_data as a list
# Create an identifier for each speech (1 to 36 in this case)
speech_ids <- 1:36
# Create an empty list to store the document-topic distributions
document_topics_list <- list()
# Loop through the list of speeches and extract the document-topic distribution
for (i in speech_ids) {
# Extract the document-topic distribution for speech i
doc_topic_dist <- document_topics$topics[, i]
document_topics_list[[i]] <- doc_topic_dist
}
# Combine the speech data with document-topic distributions
speech_data_with_topics <- data.frame(
Speech = speech_data,
SpeechID = speech_ids,
DocumentTopicDistribution = document_topics_list
)
document_topics_list
document_topics_list[1]
document_topics
# Loop through the list of speeches and extract the document-topic distribution
for (i in speech_ids) {
# Extract the document-topic distribution for speech i
doc_topic_dist <- document_topics$topic[, i]
document_topics_list[[i]] <- doc_topic_dist
}
lda_model
