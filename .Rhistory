invisible(lapply(libs, library, character.only = TRUE))
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'purrr', 'reshape2', 'text2vec', 'tidyr', 'tidytext', 'topicmodels', 'tm', 'wordcloud')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
unzip("sona-addresses-1994-2023.zip")
# Get a list of all text files in the directory
text_files <- list.files(pattern = ".txt")
# Initialize an empty list to store the data
speech_data <- list()
# Loop through the list of text files and read them into R
for (file in text_files) {
speech <- readLines(file, warn = FALSE)
speech_data[[file]] <- speech
}
#data wrangling
speech_tibble <- tibble(speech = speech_data)
speech_tibble <- speech_tibble %>%
separate(speech, into = c("day", "month" , "year","content"), sep = " ",fill= "right", extra = "merge")
speech_tibble <- speech_tibble %>%
mutate(
content = tolower(content),
content = gsub("\\d", "", content),
content = gsub("[^[:alnum:]']", " ", content),
content = gsub("[^[:alnum:]']", " ", content),
day = gsub("[c(,'’\"]", "", day),
year = gsub("[,'’\"]", "", year)
) %>% unite("Date", day:month:year, remove = TRUE, sep = " ")
speech_tibble$Date[35]<- "10 February 2022"
speech_tibble$Date[36]<- "9 February 2022"
speech_tibble$Date<- dmy(speech_tibble$Date)
# Assuming you have already loaded the 'lubridate' package
speech_tibble <- speech_tibble %>%
mutate(
President = case_when(
Date < ymd("1999-06-14") ~ "Nelson Mandela",
Date >= ymd("1999-06-14") & Date < ymd("2008-09-24") ~ "Thabo Mbeki",
Date >= ymd("2008-09-24") & Date <= ymd("2009-05-09") ~ "Kgalema Motlanthe",
Date >= ymd("2009-05-09") & Date <= ymd("2018-02-14") ~ "Jacob Zuma",
TRUE ~ "Cyril Ramaphosa"
)
)
speech_tokens <- speech_tibble %>%
unnest_tokens(word, content) %>%
anti_join(stop_words)
words_to_remove <- c("government", "South Africa", "national",
"country", "south", "africa", "honourable",
"people")
speech_tokens <- speech_tokens %>%
filter(!word %in% words_to_remove)
load("dsfi-lexicons.Rdata")
#data wrangling
speech_tibble <- tibble(speech = speech_data)
speech_tibble <- speech_tibble %>%
separate(speech, into = c("day", "month" , "year","content"), sep = " ",fill= "right", extra = "merge")
speech_tibble <- speech_tibble %>%
mutate(
content = tolower(content),
content = gsub("\\d", "", content),
content = gsub("[^[:alnum:]']", " ", content),
content = gsub("[^[:alnum:]']", " ", content),
day = gsub("[c(,'’\"]", "", day),
year = gsub("[,'’\"]", "", year)
) %>% unite("Date", day:month:year, remove = TRUE, sep = " ")
speech_tibble$Date[35]<- "10 February 2022"
speech_tibble$Date[36]<- "9 February 2022"
speech_tibble$Date<- dmy(speech_tibble$Date)
# Assuming you have already loaded the 'lubridate' package
speech_tibble <- speech_tibble %>%
mutate(
President = case_when(
Date == ymd("1994-02-28") ~ "F.W. de Klerk",
Date > ymd("1994-02-28") & Date < ymd("1999-06-14") ~ "Nelson Mandela",
Date >= ymd("1999-06-14") & Date < ymd("2008-09-24") ~ "Thabo Mbeki",
Date >= ymd("2008-09-24") & Date <= ymd("2009-05-09") ~ "Kgalema Motlanthe",
Date >= ymd("2009-05-09") & Date <= ymd("2018-02-14") ~ "Jacob Zuma",
TRUE ~ "Cyril Ramaphosa"
)
)
speech_tokens <- speech_tibble %>%
unnest_tokens(word, content) %>%
anti_join(stop_words)
words_to_remove <- c("government", "South Africa", "national",
"country", "south", "africa", "honourable",
"people")
speech_tokens <- speech_tokens %>%
filter(!word %in% words_to_remove)
load("dsfi-lexicons.Rdata")
speech_sentiment <- speech_tokens %>%
left_join(bing, by = "word") %>%
rename(bing_sentiment = sentiment) %>%
mutate(bing_sentiment = ifelse(is.na(bing_sentiment), 'neutral', bing_sentiment))
#5 most positive words for each movie
speech_sentiment %>%
filter(bing_sentiment == 'positive') %>%
count(President, word) %>%
group_by(President) %>% filter(rank(desc(n)) <= 5) %>%
ggplot(aes(reorder(word,n),n)) + geom_col() + facet_wrap(~President)+
coord_flip() + xlab('')
#5 most negative words for each movie
speech_sentiment %>%
filter(bing_sentiment == 'negative') %>%
count(President, word) %>%
group_by(President) %>% filter(rank(desc(n)) <= 5) %>%
ggplot(aes(reorder(word,n),n)) + geom_col() + facet_wrap(~President) + coord_flip() + xlab('')
speech_sentiment <- speech_sentiment %>%
left_join(nrc, by = "word") %>%
rename(nrc_sentiment = sentiment)
speech_sentiment%>%
add_count(President,Date, name = "n_words") %>%
na.omit() %>%
group_by(President,Date, nrc_sentiment) %>%
summarize(prop = n() / first(n_words)) %>% ungroup() %>%
group_by(President, nrc_sentiment) %>%
summarize(mean_prop = mean(prop)) %>% ungroup() %>%
ggplot(aes(reorder(nrc_sentiment, mean_prop), mean_prop, fill = President)) +
geom_bar(stat = "identity", position = 'dodge') + coord_flip() + xlab('')
speech_tdf <- speech_tokens%>%
group_by(Date,word) %>%
count() %>%
ungroup()
dtm_speech <- speech_tdf %>%
cast_dtm(Date, word, n)
speech_lda <- LDA(dtm_speech, k = 36, control = list(seed = 2023))
speech_topics <- tidy(speech_lda, matrix = 'beta')
speech_topics %>%
group_by(topic) %>%
slice_max(n = 1, order_by = beta) %>% ungroup() %>%
arrange(topic, -beta) %>%
ggplot(aes(reorder(term, beta), beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = 'free') + coord_flip()
speech_tdf_presidents <- speech_tokens%>%
group_by(President,word) %>%
count() %>%
ungroup()
dtm_speech_presidents <- speech_tdf_presidents %>%
cast_dtm(President, word, n)
speech_lda_presidents<- LDA(dtm_speech_presidents, k = 5, control = list(seed = 2023))
speech_topics_presidents <- tidy(speech_lda_presidents, matrix = 'beta')
speech_topics_presidents %>%
group_by(topic) %>%
slice_max(n = 20, order_by = beta) %>% ungroup() %>%
arrange(topic, -beta) %>%
ggplot(aes(reorder(term, beta), beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = 'free') + coord_flip()
knitr::opts_knit$set(root.dir = "C:/Users/User/OneDrive/Documents/School/2023/Masters/STA5073Z/Assignments/Assignment 1/ds4l-assignment-1/data/")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, fig.pos = "H", out.extra = "")
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'purrr', 'reshape2', 'text2vec', 'tidyr', 'tidytext', 'topicmodels', 'tm', 'wordcloud')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
# data wrangling
#data wrangling
speech_tibble <- tibble(speech = speech_data)
# Get a list of all text files in the directory
text_files <- list.files(pattern = ".txt")
# Initialize an empty list to store the data
speech_data <- list()
# Loop through the list of text files and read them into R
for (file in text_files) {
speech <- readLines(file, warn = FALSE)
speech_data[[file]] <- speech
}
# data wrangling
#data wrangling
speech_tibble <- tibble(speech = speech_data)
speech_tibble <- speech_tibble %>%
separate(speech, into = c("day", "month" , "year","content"), sep = " ",fill= "right", extra = "merge")
speech_tibble <- speech_tibble %>%
mutate(
content = tolower(content),
content = gsub("\\d", "", content),
content = gsub("[^[:alnum:]']", " ", content),
content = gsub("[^[:alnum:]']", " ", content),
day = gsub("[c(,'’\"]", "", day),
year = gsub("[,'’\"]", "", year)
) %>% unite("Date", day:month:year, remove = TRUE, sep = " ")
speech_tibble$Date[35]<- "10 February 2022"
speech_tibble$Date[36]<- "9 February 2022"
speech_tibble$Date<- dmy(speech_tibble$Date)
# Assuming you have already loaded the 'lubridate' package
speech_tibble <- speech_tibble %>%
mutate(
President = case_when(
Date == ymd("1994-02-28") ~ "F.W. de Klerk",
Date > ymd("1994-02-28") & Date < ymd("1999-06-14") ~ "Nelson Mandela",
Date >= ymd("1999-06-14") & Date < ymd("2008-09-24") ~ "Thabo Mbeki",
Date >= ymd("2008-09-24") & Date <= ymd("2009-05-09") ~ "Kgalema Motlanthe",
Date >= ymd("2009-05-09") & Date <= ymd("2018-02-14") ~ "Jacob Zuma",
TRUE ~ "Cyril Ramaphosa"
)
)
speech_tokens <- speech_tibble %>%
unnest_tokens(word, content) %>%
anti_join(stop_words)
words_to_remove <- c("government", "South Africa", "national",
"country", "south", "africa", "honourable",
"people")
speech_tokens <- speech_tokens %>%
filter(!word %in% words_to_remove)
load("dsfi-lexicons.Rdata")
load("C:/Users/User/OneDrive/Documents/School/2023/Masters/STA5073Z/Lecture 04 - Neural Networks/trump_tfidf.RData")
knitr::opts_knit$set(root.dir = "C:/Users/User/OneDrive/Documents/School/2023/Masters/STA5073Z/Assignments/Assignment 2/ds4l-assignment-2/")
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'purrr', 'quarto', 'reshape2', 'stringr', 'text2vec', 'tidyr', 'tidytext', 'topicmodels', 'tm', 'wordcloud')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
unzip("sona-addresses-1994-2023.zip", exdir = "data")
knitr::opts_knit$set(root.dir = "C:/Users/User/OneDrive/Documents/School/2023/Masters/STA5073Z/Assignments/Assignment 2/ds4l-assignment-2/")
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'knitr', 'lubridate', 'purrr', 'quarto', 'reshape2', 'stringr', 'text2vec', 'tidyr', 'tidytext', 'topicmodels', 'tm', 'wordcloud')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
unzip("sona-addresses-1994-2023.zip", exdir = "data")
# Get a list of all text files in the directory
text_files <- list.files(path = "data", pattern = ".txt")
# filenames <- purrr::flatten(text_files)
# Initialize an empty list to store the data
# speech_data <- list()
speech_data <- c()
i = 0
num_chars <- c(27050, 12786, 39019, 39524, 37489, 45247, 34674, 41225, 37552, 41719, 50544, 58284, 34590, 39232, 54635, 48643, 48641, 44907, 31101, 47157, 26384, 33281, 33376, 36006, 29403, 36233, 32860, 32464, 35981, 33290, 42112, 56960, 47910, 43352, 52972, 60000)
# Loop through the list of text files and read them into R
for (file in text_files) {
i = i + 1
# speech <- readLines(file, warn = FALSE)
# Open the file for reading
file_handle <- file(paste("data/", file, sep = ""), "r")
speech <- readChar(file_handle, nchars = num_chars[i])
# speech_data[[file]] <- speech
speech_data[i] <- speech
# Close the file
close(file_handle)
}
sona <- data.frame(filename = text_files, speech = speech_data, stringsAsFactors = FALSE)
# extract year and president for each speech
sona$year <- str_sub(sona$filename, start = 1, end = 4)
sona$president <- str_remove_all(str_extract(sona$filename, "[dA-Z].*\\."), "\\.")
# clean the sona dataset by adding the date and removing unnecessary text
replace_reg <- '(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\n'
unnest_reg <- "[^A-Za-z_\\d#@']"
sona <-sona %>%
mutate(speech = str_replace_all(speech, replace_reg , ' ')
,date = str_sub(speech, start=1, end=30)
,date = str_replace_all(date, "February", "02")
,date = str_replace_all(date, "June", "06")
,date = str_replace_all(date, "Feb", "02")
,date = str_replace_all(date, "May", "05")
,date = str_replace_all(date, "Jun", "06")
,date = str_replace_all(date, "Thursday, ","")
,date = str_replace_all(date, ' ', '-')
,date = str_replace_all(date, "[A-z]",'')
,date = str_replace_all(date, '-----', '')
,date = str_replace_all(date, '----', '')
,date = str_replace_all(date, '---', '')
,date = str_replace_all(date, '--', '')
) %>%
mutate(speech = str_replace_all(speech, replace_reg, ""))
sona$date[36] <- "09-02-2023"
sona$year[36] <- "2023"
sona$date <- dmy(sona$date)
# Tokenize
speech_tokens <- sona %>%
unnest_tokens(word, speech, token = "regex", pattern = unnest_reg) %>%
anti_join(stop_words)
words_to_remove <- c("government", "South Africa", "national",
"country", "south", "africa", "honourable",
"people")
speech_tokens <- speech_tokens %>%
filter(!word %in% words_to_remove)
load("dsfi-lexicons.Rdata")
#to be put in the appendices
set.seed(2023)
bing_sample <- t(bing[sample(1:6786, 10),])
kable(bing_sample, caption = "Sample of bing lexicon")
set.seed(2023)
nrc_sample <- t(nrc[sample(1:6786, 10),])
kable(nrc_sample, caption = "Sample of nrc lexicon")
speech_sentiment <- speech_tokens %>%
left_join(bing, by = "word") %>%
rename(bing_sentiment = sentiment) %>%
mutate(bing_sentiment = ifelse(is.na(bing_sentiment), 'neutral', bing_sentiment))
#| label: fig-positive
#| fig-cap: "Most positive words used by each president"
speech_sentiment %>%
filter(bing_sentiment == 'positive') %>%
count(president, word) %>%
group_by(president) %>% filter(rank(desc(n)) <= 5) %>%
ggplot(aes(reorder(word,n),n)) + geom_col(aes(fill = word)) + facet_wrap(~president)+
coord_flip() + labs( y = 'Contribution to Sentiment')
#| label: fig-nwords
#| fig-cap: "Most Negative words used by each president"
speech_sentiment %>%
filter(bing_sentiment == 'negative') %>%
count(president, word) %>%
group_by(president) %>% filter(rank(desc(n)) <= 5) %>%
ggplot(aes(reorder(word,n),n)) + geom_col() + facet_wrap(~president) + coord_flip() + xlab('')
speech_sentiment <- speech_sentiment %>%
left_join(nrc, by = "word")  %>%
rename(nrc_sentiment = sentiment)
speech_sentiment%>%
count(president, word) %>%
group_by(president) %>% filter(rank(desc(n)) <= 5) %>%
ggplot(aes(reorder(word,n),n)) + geom_col(aes(fill =word)) + facet_wrap(~president) + coord_flip() + labs( y = 'Contribution to Sentiment')
#| label: fig-overall
#| fig-cap: "Proportion of sentiment words in speeches for each president"
speech_sentiment%>%
add_count(president,date, name = "n_words") %>%
na.omit() %>%
group_by(president,date, nrc_sentiment) %>%
summarize(mean_prop = n() / first(n_words)) %>% ungroup() %>%
ggplot(aes(reorder(nrc_sentiment, mean_prop), mean_prop, fill = president)) +
geom_bar(stat = "identity", position = 'dodge') + coord_flip() + xlab('')
#| label: fig-sentimentscore
#| fig-cap: "Time series Plot of Average Sentiment using AFINN Lexicon Scoring System. Higher Values Represent more Positive Sentiment"
speech_sentiment <- speech_sentiment %>%
left_join(afinn, by = "word") %>%
rename(afinn_value = value)
speech_sentiment%>%
select(date, president, word, afinn_value)%>%
na.omit%>%
group_by( date, president) %>%
summarise(.groups = "drop", Mean_Score = mean(afinn_value)) %>%
ungroup() %>%
ggplot(aes(x = date, y = Mean_Score, col = president))+ geom_line() +
labs(title = "Average Sentiment Score Time Series", x = "date", y = " Average Sentiment Score") +
theme_minimal()
speech_tdf <- speech_tokens%>%
group_by(date,word) %>%
count() %>%
ungroup()
dtm_speech <- speech_tdf %>%
cast_dtm(date, word, n)
speech_lda <- LDA(dtm_speech, k = 36, control = list(seed = 2023))
speech_topics <- tidy(speech_lda, matrix = 'beta')
speech_topics %>%
group_by(topic) %>%
slice_max(n = 1, order_by = beta) %>% ungroup() %>%
arrange(topic, -beta) %>%
ggplot(aes(reorder(term, beta), beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = 'free') + coord_flip()
tidy_speeches <- sona %>%
unnest_tokens(word, speech, token = "words", to_lower = T) %>%
filter(!word %in% stop_words$word)
speech_tdf <- tidy_speeches%>%
group_by(date,word) %>%
count() %>%
ungroup()
dtm_speech <- speech_tdf %>%
cast_dtm(date, word, n)
speech_lda <- LDA(dtm_speech, k = 5, control = list(seed = 2023))
speech_topics <- tidy(speech_lda, matrix = 'beta')
speech_topics %>%
group_by(topic) %>%
slice_max(n = 1, order_by = beta) %>% ungroup() %>%
arrange(topic, -beta) %>%
ggplot(aes(reorder(term, beta), beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = 'free') + coord_flip()
term <- as.character(speech_lda@terms)
topic1 <- speech_lda@beta[1,]
topic2 <- speech_lda@beta[2,]
topic3 <- speech_lda@beta[3,]
topic4 <- speech_lda@beta[4,]
topic5 <- speech_lda@beta[5,]
reviews_topics <- tibble(term = term, topic1 = topic1, topic2 = topic2, topic3 = topic3, topic4 = topic4, topic5 = topic5)
speech_topics <- speech_topics %>%
gather(topic1, topic2, topic3, topic4, topic5, key = "topic", value = "beta") %>%
mutate(beta = exp(beta)) # pr(topic k generates word i) = exp(beta_ik)
speech_lda <- LDA(dtm_speech, k = 2, control = list(seed = 2023))
reviews_topics <- tibble(term = term, topic1 = topic1, topic2 = topic2)
speech_topics <- speech_topics %>%
gather(topic1, topic2, key = "topic", value = "beta") %>%
mutate(beta = exp(beta)) # pr(topic k generates word i) = exp(beta_ik)
speech_topics <- tibble(term = term, topic1 = topic1, topic2 = topic2)
speech_topics <- speech_topics %>%
gather(topic1, topic2, key = "topic", value = "beta") %>%
mutate(beta = exp(beta)) # pr(topic k generates word i) = exp(beta_ik)
head(speech_topics)
topic1
speech_topics <- tidy(speech_lda, matrix = 'beta')
speech_topics
speech_lda <- LDA(dtm_speech, k = 5, control = list(seed = 2023))
speech_topics <- tidy(speech_lda, matrix = 'beta')
speech_topics %>%
group_by(topic) %>%
slice_max(n = 1, order_by = beta) %>% ungroup() %>%
arrange(topic, -beta) %>%
ggplot(aes(reorder(term, beta), beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = 'free') + coord_flip()
tidy_speeches <- sona %>%
unnest_tokens(word, speech, token = "words", to_lower = T) %>%
filter(!word %in% stop_words$word) %>%
filter(!word %in% words_to_remove)
dtm_speech <- speech_tdf %>%
cast_dtm(date, word, n)
speech_lda <- LDA(dtm_speech, k = 5, control = list(seed = 2023))
speech_topics <- tidy(speech_lda, matrix = 'beta')
speech_topics %>%
group_by(topic) %>%
slice_max(n = 1, order_by = beta) %>% ungroup() %>%
arrange(topic, -beta) %>%
ggplot(aes(reorder(term, beta), beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = 'free') + coord_flip()
words_to_remove <- c("government", "South Africa", "national",                      "country", "south", "africa", "honourable", "people")
tidy_speeches <- sona %>%
unnest_tokens(word, speech, token = "words", to_lower = T) %>%
filter(!word %in% stop_words$word) %>%
filter(!word %in% words_to_remove)
speech_tdf <- tidy_speeches%>%
group_by(date,word) %>%
count() %>%
ungroup()
dtm_speech <- speech_tdf %>%
cast_dtm(date, word, n)
speech_lda <- LDA(dtm_speech, k = 5, control = list(seed = 2023))
speech_topics <- tidy(speech_lda, matrix = 'beta')
speech_topics %>%
group_by(topic) %>%
slice_max(n = 1, order_by = beta) %>% ungroup() %>%
arrange(topic, -beta) %>%
ggplot(aes(reorder(term, beta), beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = 'free') + coord_flip()
top_terms <- speech_topics %>%
group_by(topic) %>%
top_n(20, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
beta_spread <- speech_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1))
beta_spread %>%
group_by(direction = log_ratio > 0) %>%
top_n(10, abs(log_ratio)) %>%
ungroup() %>%
mutate(term = reorder(term, log_ratio)) %>%
ggplot(aes(term, log_ratio)) +
geom_col() +
labs(y = "Log2 ratio of beta in topic 2 / topic 1") +
coord_flip()
tidy(speech_lda, matrix = "gamma")
speeches_gamma <- sona %>%
left_join(tidy(reviews_lda, matrix = "gamma") %>%
mutate(speechId = as.numeric(document)) %>% # some cleaning to make key variable (reviewId) usable
select(-document) %>%
spread(key = topic, value = gamma, sep = "_"))
speeches_gamma <- sona %>%
left_join(tidy(speech_lda, matrix = "gamma") %>%
mutate(speechId = as.numeric(document)) %>% # some cleaning to make key variable (reviewId) usable
select(-document) %>%
spread(key = topic, value = gamma, sep = "_"))
?left_join
sona$speechId <- 1:nrow(sona)
speeches_gamma <- sona %>%
left_join(tidy(speech_lda, matrix = "gamma") %>%
mutate(speechId = as.numeric(document)) %>% # some cleaning to make key variable (reviewId) usable
select(-document) %>%
spread(key = topic, value = gamma, sep = "_"))
speeches_gamma
speeches_gamma %>%
group_by(imdbId) %>%
summarize(ntopic1 = sum(topic_1 > 0.5))
speeches_gamma %>%
group_by(speechId) %>%
summarize(ntopic1 = sum(topic_1 > 0.5))
speeches_gamma %>%
group_by(speechId) %>%
summarize(ntopic1 = sum(topic_1 > 0.01))
